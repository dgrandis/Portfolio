{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Теплов - Pro- Обработка текстов с помощью нейросетей.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DkGGagPu4I-y"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cATy17YxtqEA"
      },
      "source": [
        "**Задание Pro**\r\n",
        "\r\n",
        "Добейтесь точности распознавания 97% и верно распознанных всех писателей с помощью любой нейронной сети без фильтрации данных. Попробуйте менять размер окна и шаг"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-GQT24B4qPQ"
      },
      "source": [
        "# Подключение библиотек "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNvBtaWGtdXg"
      },
      "source": [
        "# Для начала работы подключим все необходимые библиотеки\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dropout, Dense,  BatchNormalization, Activation\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras import utils\r\n",
        "from google.colab import drive\r\n",
        "import numpy as np\r\n",
        "from google.colab import files\r\n",
        "import gc \r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkGGagPu4I-y"
      },
      "source": [
        "# Объявим все необходимые функции"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuKTe6hZCmJH"
      },
      "source": [
        "# Создадим функцию для первоначальной обработки файла\r\n",
        "def readText(fileName):\r\n",
        "  op = open(fileName, 'r')        # Указываем что открытие будет в режиме чтения\r\n",
        "  text = op.read()                # Читаем файл с текстом\r\n",
        "  text = text.replace(\"\\n\", \" \")  # Заменям переносы строк на пробелы\r\n",
        "  return text                     # Возвращаем измененный файл"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3075MhUNxRj2"
      },
      "source": [
        "# Для начала создадим функции. \r\n",
        "# Первая для разделения на короткие векторы с заданным шагом и размером\r\n",
        "\r\n",
        "def getSetFromIndexes (wordIndexes, xLen, step):     # В качестве аргументов принимает (trainWordIndexes или testWordIndexes - список, размер и шаг) \r\n",
        "  xSample = []                                        # Созадим пустой спискок для хранения векторов\r\n",
        "  wordsLen = len(wordIndexes)                         # Кол-во слов в поданных на вход trainWordIndexes или testWordIndexes\r\n",
        "  index = 0                                           # Начальный индекс (с помощью этой переменной будем сдвигать шаг)\r\n",
        "\r\n",
        "\r\n",
        "  while (index + xLen <= wordsLen):                   # Задаем цикл пока не дойдем до конца    \r\n",
        "    xSample.append(wordIndexes[index:index + xLen])   # На каждой итерации добавляем в список кусок вектора заданной длины\r\n",
        "    index += step                                     # Смещаемся на шаг\r\n",
        "\r\n",
        "  return xSample"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeUE6RwDxZJg"
      },
      "source": [
        "# Вторая функция для формирования обучающей и проверочной выборок\r\n",
        "\r\n",
        "def createSetsMultiClasses(wordIndexes, xLen, step):\r\n",
        "  nClasses = len(wordIndexes)                            # Зададим кол-во классов выборки\r\n",
        "  classesXSample = []                                    # Создадим пустой список в котором будут храниться список нарезанных кусочков предыдущей функцией getSetFromIndexes\r\n",
        "\r\n",
        "  # Пройдемся циклом по каждому тексту выборки из последовательнстей индексов\r\n",
        "  for i in wordIndexes:\r\n",
        "    classesXSample.append(getSetFromIndexes(i, xLen, step))\r\n",
        "  \r\n",
        "  # Сформируем один общий список\r\n",
        "  xSamples = []                                           # Создаем пустые списки\r\n",
        "  ySamples = []\r\n",
        "\r\n",
        "  for t in range(nClasses):                               # В нашем случаи цикл от 0 до 6 (кол-во классов)\r\n",
        "    xT = classesXSample[t]                                # На каждой итерации цикла берем очередной нарезанный текст одного из класса\r\n",
        "\r\n",
        "    for i in range (len(xT)):                             # Циклом проходимся по каждому его окну\r\n",
        "      xSamples.append(xT[i])                              # Добавляем в общий список обучающей выборке\r\n",
        "      ySamples.append(utils.to_categorical(t, nClasses))  # И соответствующий вектор класса\r\n",
        "  \r\n",
        "  # Переводим в numpy массив\r\n",
        "  xSamples = np.array(xSamples)                           \r\n",
        "  ySamples = np.array(ySamples)    \r\n",
        "\r\n",
        "  # B возвращаем выборки\r\n",
        "  return (xSamples, ySamples)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mndi_l1HoShQ"
      },
      "source": [
        "# Создадим функцию которая будет представлять выборку в удобном для распознавания размерах\r\n",
        "\r\n",
        "def createTestMultiClasses(wordIndexes, xLen, step):\r\n",
        "\r\n",
        "  # Создаем тестовую выборку из индексов для каждого из 6 классов\r\n",
        "  nClasses = len(wordIndexes)                                             # Переменной nClasses присвоим кол-во классов\r\n",
        "  xTest6ClassesBow = []                                                   # Создадим пустой список, в котором будут список из всех классов\r\n",
        "  xTest6Classec = []                                                      # Создадим список массивов\r\n",
        "\r\n",
        "  # Пройдемся циклом по каждому тестовому тексту из последовательности индексов\r\n",
        "  for i in wordIndexes:\r\n",
        "    sample = (getSetFromIndexes(i,xLen, step))                            # Воспользуемся ранее написанной функцией для нарезки на векторы\r\n",
        "    xTest6Classec.append(sample)                                          # Добавляем в список на каждой итерации \r\n",
        "    xTest6ClassesBow.append(tokenizer.sequences_to_matrix(sample))        # Трансформируем в BOW\r\n",
        "  \r\n",
        "  xTest6ClassesBow = np.array(xTest6ClassesBow)                           # Переводим в numpy массив\r\n",
        "  xTest6Classec = np.array(xTest6Classec)                                 # Переводим в numpy массив\r\n",
        "\r\n",
        "  return xTest6ClassesBow, xTest6Classec"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzKrKxi9Wmfd"
      },
      "source": [
        "# Создадим еще одну функцию которая будет распознавать тестовую выборку и выводить результаты\r\n",
        "def recognizerMultiClass (model, xTest, modelName):\r\n",
        "  #print (\"Название нейронной сети: \", modelName)        # Для наглядности выведем название сети\r\n",
        "  #print ()                                              # Пропустим стороку\r\n",
        "  totalSumRec = 0                                       # Создадим счетчик суммы правильных ответов\r\n",
        "\r\n",
        "  # Пройдемся циклом по всем классам\r\n",
        "  for i in range(nClasses):                             # От 0 до 6 \r\n",
        "    currPred = model.predict(xTest[i])                  # Получим результаты распознования класса по блокам слов длинны xLen.\r\n",
        "    currOut = np.argmax(currPred, axis=1)               # Определим номер распознанного класса с помощью функции argmax\r\n",
        "    evVal = []\r\n",
        "    \r\n",
        "    for j in range(nClasses):\r\n",
        "      evVal.append(len(currOut[currOut==j])/len(xTest[i]))\r\n",
        "\r\n",
        "    totalSumRec +=len(currOut[currOut==i])\r\n",
        "    recognizedClass = np.argmax(evVal)                  # Определяем какой класс за какой был распознан в итоге\r\n",
        "\r\n",
        "    # И выводим результаты\r\n",
        "\r\n",
        "    isRecognized = \"Ответ НЕВЕРНЫЙ, УВЫ\"\r\n",
        "    if (recognizedClass == i):\r\n",
        "      isRecognized = \"УРА, ответ ВЕРНЫЙ!\"\r\n",
        "    \r\n",
        "    str1 = 'Класс: ' + className[i] + \" \" * (11 - len(className[i])) + str(int(100*evVal[i])) + \"% сеть предсказала: \" + className[recognizedClass]\r\n",
        "    print(str1, \" \" * (55-len(str1)), isRecognized, sep='')\r\n",
        "\r\n",
        "  # Выведим средний процент распознавания \r\n",
        "  print ()                                                                          # Пустая строка чтобы отделить\r\n",
        "  sumCount = 0\r\n",
        "  # Пройдемся циклом по всем циклам\r\n",
        "  for i in range(nClasses):\r\n",
        "    sumCount += len(xTest[i])\r\n",
        "  print (\"Средний процент распознавания \", int(100*totalSumRec/sumCount), \"%\")\r\n",
        "  print ()                                                                          # Пустая строка чтобы отделить\r\n",
        "\r\n",
        "  return totalSumRec/sumCount"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfih5mcLD6-r"
      },
      "source": [
        "# Подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm_Lqah8RGLc"
      },
      "source": [
        "# Объявим классы. Запишем в список\r\n",
        "className = [\"О. Генри\", \"Стругацкие\", \"Булгаков\", \"Саймак\", \"Фрай\", \"Брэдберри\"]\r\n",
        "\r\n",
        "# Подсчитаем кол-во нужных нам классов\r\n",
        "nClasses = len(className)\r\n",
        "\r\n",
        "# Создадим 2 пустых списка (обучающий и проверочный), для последующего заполнения\r\n",
        "trainText = []\r\n",
        "testText = []"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt4Qv238WXbd",
        "outputId": "5f1acf07-73e8-4316-8aec-a1dfcb276f03"
      },
      "source": [
        "# Пройдемся двумя циклами для заполнения списков. Так чтобы каждый i элемент в обучающей выборке соответствовал j элементу в тестовой.\r\n",
        "\r\n",
        "for i in className:\r\n",
        "  for j in os.listdir('/content/'):                     \r\n",
        "    if i in j:                                       \r\n",
        "      if 'Обучающая' in j:                           \r\n",
        "        trainText.append(readText('/content/' +j))      \r\n",
        "        print (j, 'добавлен в обучающую выборку')    \r\n",
        "      if 'Тестовая' in j:                            \r\n",
        "        testText.append(readText('/content/' +j))       \r\n",
        "        print (j, 'добавлен в тестовую выборку')     "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(О. Генри) Тестовая_20 вместе.txt добавлен в тестовую выборку\n",
            "(О. Генри) Обучающая_50 вместе.txt добавлен в обучающую выборку\n",
            "(Стругацкие) Тестовая_2 вместе.txt добавлен в тестовую выборку\n",
            "(Стругацкие) Обучающая_5 вместе.txt добавлен в обучающую выборку\n",
            "(Булгаков) Обучающая_5 вместе.txt добавлен в обучающую выборку\n",
            "(Булгаков) Тестовая_2 вместе.txt добавлен в тестовую выборку\n",
            "(Клиффорд_Саймак) Тестовая_2 вместе.txt добавлен в тестовую выборку\n",
            "(Клиффорд_Саймак) Обучающая_5 вместе.txt добавлен в обучающую выборку\n",
            "(Макс Фрай) Тестовая_2 вместе.txt добавлен в тестовую выборку\n",
            "(Макс Фрай) Обучающая_5 вместе.txt добавлен в обучающую выборку\n",
            "(Рэй Брэдберри) Обучающая_22 вместе.txt добавлен в обучающую выборку\n",
            "(Рэй Брэдберри) Тестовая_8 вместе.txt добавлен в тестовую выборку\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpmWiQ41Hjmq"
      },
      "source": [
        "# Зададим значения которые будут неизменными во всем ноутбуке.\r\n",
        "\r\n",
        "maxWordsCount = 15000\r\n",
        "xLen = 5000                              # Длина отрезка текста на которые будет разбивать функция getSetFromIndexes\r\n",
        "step = 100                               # Шаг разбивания для той же функции"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZAU-1Of1X02"
      },
      "source": [
        "**Так как в заднии сказано без фильтрации данных, токенизировать будем без filters**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtm9cE_W1QGA"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = maxWordsCount, lower=True, split=' ', oov_token ='unknow', char_level = False)\r\n",
        "tokenizer.fit_on_texts(trainText)   \r\n",
        "items = list(tokenizer.word_index.items())\r\n",
        "\r\n",
        "# Согласно частотному словарю можно преобразовать текст в последовательность индексов\r\n",
        "trainWordIndexes = tokenizer.texts_to_sequences(trainText)\r\n",
        "testWordIndexes = tokenizer.texts_to_sequences(testText)   \r\n",
        "\r\n",
        "# Установим базовые параметры\r\n",
        "\r\n",
        "# Сформируем выборки (обучающую и тестовую)\r\n",
        "xTrain, yTrain = createSetsMultiClasses(trainWordIndexes, xLen, step)\r\n",
        "xTest, yTest = createSetsMultiClasses(testWordIndexes, xLen, step)\r\n",
        "\r\n",
        "# Преобразовываем в матрицы нулей и единиц по принципу BOW. xTrain и yTrain подаем в виду списка. \r\n",
        "xTrainBOW = tokenizer.sequences_to_matrix(xTrain.tolist())      \r\n",
        "xTestBOW = tokenizer.sequences_to_matrix(xTest.tolist()) "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TacyaIRxNzr"
      },
      "source": [
        "По сравнению с прошлым заданием ноутбук не на столько большой. Но подготовительной работы было много. Так как найти нужную архитектуру нейронной сети и подобрать парвильные гиперпараметры, чтобы добиться определенного результата было не просто. Изменялись такие параметры:\r\n",
        "  - Значение **maxWordsCount** в пределах от 5000 до 25000\r\n",
        "  - Длина отрезка текста на которые будет разбивать функция от 500 до 5000\r\n",
        "  - Шаг от 50 до 500\r\n",
        "  - Количество скрытых слоев:\r\n",
        "    - Добавление еще полносвязных **Dense** слоев\r\n",
        "    - Добавление слоев нормализации **BatchNormalization**\r\n",
        "    - Добавление слоя **Dropout**\r\n",
        "  - Изменение количетсва нейронов в **Dense** слоях в пределах от 10 до 1000\r\n",
        "  - Изменение параметра в слоях **Dropout** \r\n",
        "  - Изменения активационных функций\r\n",
        "  - Изменения шага обучения **lr**\r\n",
        "\r\n",
        "\r\n",
        "Оставил только нейронные сети, которые удовлетворяют показанию на тестовой выборке больше 97%, остальных вариантов было много и чтобы не засорять ноутбук в работу не включил. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tc72xZL_Cww",
        "outputId": "ac4ffd7c-cef0-4ece-ef42-3437ada00f6c"
      },
      "source": [
        "# чистим оперативную память   \r\n",
        "gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3kVq3Iwzjg2"
      },
      "source": [
        "# Вариант 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU5By3ZBzW5q"
      },
      "source": [
        "Первый раз добился результата с данной архитектурой. Но пришлось обучать данную сеть 2 раза. Первый раз с шагом обучения по умолчания lr=0.001,\r\n",
        "а второй раз уже с шагом lr=0.0001. \r\n",
        "\r\n",
        "Но зато результат больше 97%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Wd3XiQy1097",
        "outputId": "950297bb-3dbb-46a8-e326-4a5bbe284265"
      },
      "source": [
        "# Создадим полносвязную сеть\r\n",
        "modelBow = Sequential()\r\n",
        "modelBow.add(Dense(300, input_dim=maxWordsCount, activation = 'sigmoid'))   \r\n",
        "modelBow.add(Dropout(0.5))\r\n",
        "modelBow.add(BatchNormalization())\r\n",
        "\r\n",
        "modelBow.add(Dense(600, input_dim=maxWordsCount, activation = 'relu'))   \r\n",
        "modelBow.add(Dropout(0.5))\r\n",
        "modelBow.add(BatchNormalization())\r\n",
        "\r\n",
        "modelBow.add(Dense(6, activation ='softmax'))\r\n",
        "\r\n",
        "# Скомпилируем ее\r\n",
        "modelBow.compile(optimizer = Adam, loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "\r\n",
        "# Обучаем сеть на выборке, xTrainBOW\r\n",
        "history = modelBow.fit(xTrainBOW, yTrain, epochs=15, batch_size=150, validation_data=(xTestBOW, yTest), verbose=1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "122/122 [==============================] - 4s 15ms/step - loss: 0.3143 - accuracy: 0.8957 - val_loss: 0.4156 - val_accuracy: 0.9515\n",
            "Epoch 2/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 8.4462e-04 - accuracy: 1.0000 - val_loss: 0.2494 - val_accuracy: 0.9477\n",
            "Epoch 3/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 4.6049e-04 - accuracy: 1.0000 - val_loss: 0.1862 - val_accuracy: 0.9536\n",
            "Epoch 4/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 3.8021e-04 - accuracy: 1.0000 - val_loss: 0.1705 - val_accuracy: 0.9456\n",
            "Epoch 5/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 2.0022e-04 - accuracy: 1.0000 - val_loss: 0.1504 - val_accuracy: 0.9570\n",
            "Epoch 6/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 1.6134e-04 - accuracy: 1.0000 - val_loss: 0.1405 - val_accuracy: 0.9584\n",
            "Epoch 7/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 1.2379e-04 - accuracy: 1.0000 - val_loss: 0.1385 - val_accuracy: 0.9565\n",
            "Epoch 8/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 8.3038e-05 - accuracy: 1.0000 - val_loss: 0.1410 - val_accuracy: 0.9547\n",
            "Epoch 9/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 7.1282e-05 - accuracy: 1.0000 - val_loss: 0.1349 - val_accuracy: 0.9559\n",
            "Epoch 10/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 5.8232e-05 - accuracy: 1.0000 - val_loss: 0.1323 - val_accuracy: 0.9550\n",
            "Epoch 11/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 6.2585e-05 - accuracy: 1.0000 - val_loss: 0.1267 - val_accuracy: 0.9573\n",
            "Epoch 12/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 4.2781e-05 - accuracy: 1.0000 - val_loss: 0.1231 - val_accuracy: 0.9576\n",
            "Epoch 13/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 4.1242e-05 - accuracy: 1.0000 - val_loss: 0.1197 - val_accuracy: 0.9590\n",
            "Epoch 14/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 3.9725e-05 - accuracy: 1.0000 - val_loss: 0.1202 - val_accuracy: 0.9567\n",
            "Epoch 15/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 3.0988e-05 - accuracy: 1.0000 - val_loss: 0.1187 - val_accuracy: 0.9573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fiX4Rfyzm5v"
      },
      "source": [
        "**Снова обучим но с шагом lr = 0.0001**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_uc59zt_hYB",
        "outputId": "653cb6f6-2062-4153-e0b6-94eb9653e89d"
      },
      "source": [
        "# Скомпилируем ее\r\n",
        "modelBow.compile(optimizer=Adam(lr = 0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "\r\n",
        "# Обучаем сеть на выборке, xTrainBOW\r\n",
        "history = modelBow.fit(xTrainBOW, yTrain, epochs=15, batch_size=150, validation_data=(xTestBOW, yTest), verbose=1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "122/122 [==============================] - 2s 13ms/step - loss: 2.1601e-05 - accuracy: 1.0000 - val_loss: 0.1014 - val_accuracy: 0.9596\n",
            "Epoch 2/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 9.0258e-06 - accuracy: 1.0000 - val_loss: 0.0901 - val_accuracy: 0.9636\n",
            "Epoch 3/15\n",
            "122/122 [==============================] - 1s 11ms/step - loss: 8.8391e-06 - accuracy: 1.0000 - val_loss: 0.0917 - val_accuracy: 0.9611\n",
            "Epoch 4/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 8.6125e-06 - accuracy: 1.0000 - val_loss: 0.0902 - val_accuracy: 0.9581\n",
            "Epoch 5/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 5.2403e-06 - accuracy: 1.0000 - val_loss: 0.0747 - val_accuracy: 0.9716\n",
            "Epoch 6/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 3.5276e-06 - accuracy: 1.0000 - val_loss: 0.0767 - val_accuracy: 0.9694\n",
            "Epoch 7/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 4.6802e-06 - accuracy: 1.0000 - val_loss: 0.0668 - val_accuracy: 0.9756\n",
            "Epoch 8/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 2.5968e-06 - accuracy: 1.0000 - val_loss: 0.0728 - val_accuracy: 0.9716\n",
            "Epoch 9/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 1.5781e-06 - accuracy: 1.0000 - val_loss: 0.0719 - val_accuracy: 0.9721\n",
            "Epoch 10/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 1.7590e-06 - accuracy: 1.0000 - val_loss: 0.0686 - val_accuracy: 0.9737\n",
            "Epoch 11/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 1.4415e-06 - accuracy: 1.0000 - val_loss: 0.0700 - val_accuracy: 0.9724\n",
            "Epoch 12/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 2.1542e-06 - accuracy: 1.0000 - val_loss: 0.0665 - val_accuracy: 0.9743\n",
            "Epoch 13/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 1.2086e-06 - accuracy: 1.0000 - val_loss: 0.0680 - val_accuracy: 0.9734\n",
            "Epoch 14/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 1.1368e-06 - accuracy: 1.0000 - val_loss: 0.0705 - val_accuracy: 0.9706\n",
            "Epoch 15/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 7.1519e-07 - accuracy: 1.0000 - val_loss: 0.0685 - val_accuracy: 0.9712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFvuOdv_14RG",
        "outputId": "d40a1701-cab2-4182-9ed2-c2a4948b6389"
      },
      "source": [
        "# Распознавание проверочной выборки\r\n",
        "\r\n",
        "# Преобразуем тестовую выборку\r\n",
        "xTest6ClassesBow, x2 = createTestMultiClasses(testWordIndexes, xLen, step)\r\n",
        "\r\n",
        "# Проверим точность сети на BOW\r\n",
        "pred = recognizerMultiClass (modelBow, xTest6ClassesBow, \"BOW: Dense-DropOut-BatchNormalization-Dense\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Класс: О. Генри   100% сеть предсказала: О. Генри      УРА, ответ ВЕРНЫЙ!\n",
            "Класс: Стругацкие 94% сеть предсказала: Стругацкие     УРА, ответ ВЕРНЫЙ!\n",
            "Класс: Булгаков   93% сеть предсказала: Булгаков       УРА, ответ ВЕРНЫЙ!\n",
            "Класс: Саймак     100% сеть предсказала: Саймак        УРА, ответ ВЕРНЫЙ!\n",
            "Класс: Фрай       97% сеть предсказала: Фрай           УРА, ответ ВЕРНЫЙ!\n",
            "Класс: Брэдберри  100% сеть предсказала: Брэдберри     УРА, ответ ВЕРНЫЙ!\n",
            "\n",
            "Средний процент распознавания  97 %\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-_gNimtk6Ae"
      },
      "source": [
        "# Сохраним полученные веса\r\n",
        "modelBow.save_weights('loss: 7.1519e-07 - accuracy: 1.0000 - val_loss: 0.0685 - val_accuracy: 0.9712.h5')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xwd_YdkLzx4n"
      },
      "source": [
        "Ура ! Получилось. Такого результата удалось добиться изменив значения: \r\n",
        "\r\n",
        "maxWordsCount = **15000**\r\n",
        "\r\n",
        "xLen = **5000**\r\n",
        "\r\n",
        "step = **100 **          \r\n",
        "\r\n",
        "Теперь можно попробовать упростить архитектуру нейронной сети. Посмотрим что получится. \r\n",
        "   Уберем часть скрытых слоев и оставим всего 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTEedutHliv_",
        "outputId": "e8633bc8-c108-451a-9319-41b156310608"
      },
      "source": [
        "# Создадим полносвязную сеть\r\n",
        "modelBow = Sequential()\r\n",
        "modelBow.add(Dense(300, input_dim=maxWordsCount, activation = 'sigmoid'))   \r\n",
        "modelBow.add(Dropout(0.5))\r\n",
        "modelBow.add(BatchNormalization())\r\n",
        "\r\n",
        "#modelBow.add(Dense(600, input_dim=maxWordsCount, activation = 'relu'))   \r\n",
        "#modelBow.add(Dropout(0.5))\r\n",
        "#modelBow.add(BatchNormalization())\r\n",
        "\r\n",
        "modelBow.add(Dense(6, activation ='softmax'))\r\n",
        "\r\n",
        "# Скомпилируем ее\r\n",
        "modelBow.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "\r\n",
        "# Обучаем сеть на выборке, xTrainBOW\r\n",
        "history = modelBow.fit(xTrainBOW, yTrain, epochs=15, batch_size=150, validation_data=(xTestBOW, yTest), verbose=1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "122/122 [==============================] - 2s 13ms/step - loss: 0.1799 - accuracy: 0.9423 - val_loss: 0.3394 - val_accuracy: 0.9639\n",
            "Epoch 2/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 3.1406e-04 - accuracy: 1.0000 - val_loss: 0.2111 - val_accuracy: 0.9639\n",
            "Epoch 3/15\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.7381e-04 - accuracy: 1.0000 - val_loss: 0.1724 - val_accuracy: 0.9633\n",
            "Epoch 4/15\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.3758e-04 - accuracy: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.9642\n",
            "Epoch 5/15\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 1.0093e-04 - accuracy: 1.0000 - val_loss: 0.1496 - val_accuracy: 0.9663\n",
            "Epoch 6/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 7.5727e-05 - accuracy: 1.0000 - val_loss: 0.1436 - val_accuracy: 0.9674\n",
            "Epoch 7/15\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 6.5211e-05 - accuracy: 1.0000 - val_loss: 0.1407 - val_accuracy: 0.9677\n",
            "Epoch 8/15\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 5.4381e-05 - accuracy: 1.0000 - val_loss: 0.1354 - val_accuracy: 0.9694\n",
            "Epoch 9/15\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 4.9286e-05 - accuracy: 1.0000 - val_loss: 0.1335 - val_accuracy: 0.9701\n",
            "Epoch 10/15\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 3.7796e-05 - accuracy: 1.0000 - val_loss: 0.1312 - val_accuracy: 0.9703\n",
            "Epoch 11/15\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 3.7389e-05 - accuracy: 1.0000 - val_loss: 0.1277 - val_accuracy: 0.9712\n",
            "Epoch 12/15\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 3.0653e-05 - accuracy: 1.0000 - val_loss: 0.1253 - val_accuracy: 0.9715\n",
            "Epoch 13/15\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 2.9168e-05 - accuracy: 1.0000 - val_loss: 0.1224 - val_accuracy: 0.9725\n",
            "Epoch 14/15\n",
            "122/122 [==============================] - 1s 10ms/step - loss: 2.5904e-05 - accuracy: 1.0000 - val_loss: 0.1206 - val_accuracy: 0.9725\n",
            "Epoch 15/15\n",
            "122/122 [==============================] - 1s 9ms/step - loss: 2.1834e-05 - accuracy: 1.0000 - val_loss: 0.1186 - val_accuracy: 0.9731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssQ-x1gDlwvc",
        "outputId": "e13ff5c9-6341-4d15-e40e-8f32fcb4c623"
      },
      "source": [
        "# Распознавание проверочной выборки\r\n",
        "\r\n",
        "# Преобразуем тестовую выборку\r\n",
        "xTest6ClassesBow, x2 = createTestMultiClasses(testWordIndexes, xLen, step)\r\n",
        "\r\n",
        "# Проверим точность сети на BOW\r\n",
        "pred = recognizerMultiClass (modelBow, xTest6ClassesBow, \"BOW: Dense-DropOut-BatchNormalization-Dense\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Класс: О. Генри   100% сеть предсказала: О. Генри      УРА, ответ ВЕРНЫЙ!\n",
            "Класс: Стругацкие 91% сеть предсказала: Стругацкие     УРА, ответ ВЕРНЫЙ!\n",
            "Класс: Булгаков   100% сеть предсказала: Булгаков      УРА, ответ ВЕРНЫЙ!\n",
            "Класс: Саймак     100% сеть предсказала: Саймак        УРА, ответ ВЕРНЫЙ!\n",
            "Класс: Фрай       95% сеть предсказала: Фрай           УРА, ответ ВЕРНЫЙ!\n",
            "Класс: Брэдберри  100% сеть предсказала: Брэдберри     УРА, ответ ВЕРНЫЙ!\n",
            "\n",
            "Средний процент распознавания  97 %\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWqnbOtT0jJr"
      },
      "source": [
        "Получилось добиться такого же результа с меньшим количество скрытых слоев и обучая нейронную сеть один раз. С шагом обучения lr=0.001 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubVwkNn_l4Aj"
      },
      "source": [
        "# Сохраним полученные веса\r\n",
        "modelBow.save_weights('loss: 2.1834e-05 - accuracy: 1.0000 - val_loss: 0.1186 - val_accuracy: 0.9731.h5')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spm5nwUj0zRp"
      },
      "source": [
        "# Выводы:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h0LSgno04bE"
      },
      "source": [
        "1. Данного результат удалось добиться путем многих десятков экспериментов.\r\n",
        "2. Очень помогло увелить **xLen** -  Длина отрезка текста на которые будет разбивать функция. При значениях до **1000**(100,200,500) идет переполнение памяти. От** 1000** до **3000** результат в районе **90%**. Свыше **4000** результат намного лучше.\r\n",
        "3. Так же зависит и шаг разбивания. Если делать маленьким, то так же будет переполнение памяти. Слишком большой тоже влияет на результат в худшую сторону. Оптимально оказалось значение **100**.\r\n",
        "4. Нельзя не сказать о maxWordsCount. Слишком маленький словарь, меньше **1000** значительно ухудшает предстказание нейронной сети. В тоже время значение выше **25000** тоже не приводят к хорошему результату, даже наоборот. Оптимальное значенеи в пределах от **10000** до **20000**. \r\n",
        "5. Количество нейронов с полносвязном слое **Dense** тоже вносят свои коррективы. Слишком большое кол-во нейронов не около **300**.\r\n",
        "6. Так же как и кол-во скрытых слоев. Как видно с меньшим количеством нейронная сеть показала результат ничуть не хуже, чем с большим им числом.\r\n",
        "7. Значение слоя **Dropout** тоже подбиралось в хоже экспериментов. Слишком малое значение не приводило к удучшению результата. Оптимально в данном случаи оказалось значение **0.5**. \r\n",
        "8. Выбор активациооной функции также повлияло на результат. Функция **sigmoid** в данной сети зарекомендовала себя лучше чем **relu** и другие. \r\n",
        "9. В купе выбор оптимальной архитектуры и подбор всех сопутствующих гиперпараметров дали хороший результат, который удовлетворяет условию задания. \r\n",
        "10. Выбор архитектуры и подбор гиперпаметров требуют опыта, который достигается благодаря исполнению подобных заданий. "
      ]
    }
  ]
}